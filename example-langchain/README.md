# Overview

# observations
(below could be due to incorrect parameters?)
* seems to be slower than directly running the model in llamacpp (takes 30-40secs)
* also, the answers are way off than directly running

# References
* https://python.langchain.com/docs/integrations/llms/llamacpp/
* Minimal Python code for local LLM inference | by Chris Paggen | Medium[[medium.com](https://medium.com/@cpaggen/minimal-python-code-for-local-llm-inference-112782af509a)]